{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install paddlepaddle paddleocr\n",
        "!pip install easyocr\n",
        "!pip install \"python-doctr[tf]\"\n",
        "!pip install \"python-doctr[torch]\"\n",
        "!pip install \"python-doctr[torch,viz,html,contib]\"\n",
        "!pip install roboflow torch torchvision easyocr opencv-python pandas ultralytics\n",
        "!pip install transformers==4.40.0 Pillow==10.1.0 torch==2.1.2 torchvision==0.16.2 sentencepiece==0.1.99 accelerate==0.30.1 bitsandbytes==0.43.1\n",
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "id": "U5Nc4LKjDZ79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import cv2\n",
        "from paddleocr import PaddleOCR, draw_ocr\n",
        "from PIL import Image, ImageFont, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import traceback\n",
        "import easyocr\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from doctr.io import DocumentFile\n",
        "from doctr.models import ocr_predictor\n",
        "from doctr.models import ocr_predictor\n",
        "from re import L\n",
        "from ultralytics import YOLO\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import csv\n",
        "import re\n",
        "import ast"
      ],
      "metadata": {
        "id": "5l-uWX2-Degs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_paddle_ocr(img_url, out_path, result, font_path=None):\n",
        "    # Download the image from the URL\n",
        "    response = requests.get(img_url)\n",
        "    image = Image.open(BytesIO(response.content))\n",
        "\n",
        "    # Convert the image to OpenCV format (numpy array)\n",
        "    image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Extract boxes, text, and scores from the result\n",
        "    boxes = [res[0] for res in result[0]]\n",
        "    txts = [res[1][0] for res in result[0]]\n",
        "    scores = [res[1][1] for res in result[0]]\n",
        "\n",
        "    # Use the provided font path, or fall back to a default font\n",
        "    try:\n",
        "        font = ImageFont.truetype(font_path, 20) if font_path else ImageFont.load_default()\n",
        "    except IOError:\n",
        "        print(\"Font file not found. Using default font.\")\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    # Draw the OCR results on the image\n",
        "    im_show = draw_ocr(image, boxes, txts, scores, font_path=font_path)\n",
        "\n",
        "    # Convert the result back to PIL format\n",
        "    im_show = Image.fromarray(im_show)\n",
        "\n",
        "    # Save the image with OCR annotations\n",
        "    im_show.save(out_path)\n",
        "\n",
        "    # Display the image\n",
        "    plt.imshow(im_show)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "uAGqm4uLEELk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tHzsmH0IDFl3"
      },
      "outputs": [],
      "source": [
        "def paddle_ocr(dataset):\n",
        "  # Initialize OCR\n",
        "  ocr = PaddleOCR(use_angle_cls=True, lang = 'en')\n",
        "\n",
        "  final_extract = []\n",
        "  for i in tqdm(range(131189)):  # Adjusted range to reflect 520 to 1000 directly\n",
        "      img_url = dataset['image_link'][i]\n",
        "\n",
        "      try:\n",
        "          result = ocr.ocr(img_url)\n",
        "\n",
        "          if result != [None]:\n",
        "              text_confidence_list = [(res[1][0], res[1][1]) for res in result[0]]\n",
        "          else:\n",
        "              text_confidence_list = []\n",
        "\n",
        "      except SystemExit:\n",
        "          # Catch SystemExit exception and continue (prevents stopping the loop)\n",
        "          print(f\"Skipping image at index {i} due to SystemExit error.\")\n",
        "          text_confidence_list = []\n",
        "\n",
        "      except Exception as e:\n",
        "          # Catch any other exceptions and log the error without stopping\n",
        "          print(f\"Error processing image at index {i}: {str(e)}\")\n",
        "          print(traceback.format_exc())  # Optionally log the full traceback\n",
        "          text_confidence_list = []\n",
        "\n",
        "      final_extract.append(text_confidence_list)\n",
        "\n",
        "      # Remove the temporary 'tmp.jpg' file if it exists\n",
        "      tmp_file = 'tmp.jpg'  # Path to the temporary file\n",
        "      if os.path.exists(tmp_file):\n",
        "          os.remove(tmp_file)\n",
        "\n",
        "  df = pd.DataFrame(list(zip(dataset['index'], dataset['image_link'][87458:], dataset['group_id'][87458:], dataset['entity_name'][87458:], dataset['entity_value'][87458:], final_extract)), columns = ['index', 'image_link', 'group_id', 'entity_name', 'entity_value', 'extracted_text'])\n",
        "  df.to_csv('paddleocr.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_image_easy(url):\n",
        "    try:\n",
        "        # Log the URL to check if it's valid\n",
        "        print(f\"Processing image from URL: {url}\")\n",
        "\n",
        "        response = requests.get(url, timeout=10)  # Add a timeout for network requests\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Error: Received response code {response.status_code} for URL {url}\")\n",
        "            return \"\"\n",
        "\n",
        "        image_arr = np.array(bytearray(response.content), dtype=np.uint8)\n",
        "        img_1 = cv2.imdecode(image_arr, cv2.IMREAD_COLOR)\n",
        "        if img_1 is None:\n",
        "            print(f\"Error: Failed to decode image from URL {url}\")\n",
        "            return \"\"\n",
        "\n",
        "        item = reader.readtext(img_1, detail=1)\n",
        "        extracted_text_with_confidence = [(entry[1], entry[2]) for entry in item]\n",
        "\n",
        "        # Log the extracted text for each image\n",
        "        print(f\"Extracted text: {extracted_text_with_confidence}\")\n",
        "\n",
        "        return extracted_text_with_confidence\n",
        "    except Exception as e:\n",
        "        # Log the exception and return an empty string to continue the process\n",
        "        print(f\"Error processing image from URL {url}: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "zTjdgVuoF4IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = easyocr.Reader(['en'], gpu=True)  # Set gpu=True if you have GPU"
      ],
      "metadata": {
        "id": "Mly81AKZGUay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_row_easy(row):\n",
        "    try:\n",
        "        # Try to extract the text from the image\n",
        "        row['Extracted_Text'] = extract_text_from_image_easy(row['image_link'])\n",
        "    except Exception as e:\n",
        "        # Handle any exceptions in row processing and return the row with empty Extracted_Text\n",
        "        print(f\"Error processing row {row.name}: {e}\")\n",
        "        row['Extracted_Text'] = \"\"\n",
        "    return row"
      ],
      "metadata": {
        "id": "lEFqZMGmF8pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_csv_range_easy(csv_path, start_row=None, end_row=None):\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Slice the DataFrame to the specified range\n",
        "    if start_row is not None and end_row is not None:\n",
        "        df = df.iloc[start_row:end_row]\n",
        "    elif start_row is not None:\n",
        "        df = df.iloc[start_row:]  # Process from start_row to the end if end_row is not specified\n",
        "    elif end_row is not None:\n",
        "        df = df.iloc[:end_row]  # Process from the beginning to end_row if start_row is not specified\n",
        "\n",
        "    # Process rows sequentially for debugging\n",
        "    processed_rows = []\n",
        "    for _, row in tqdm(df.iterrows()):\n",
        "        processed_row = process_row_easy(row)\n",
        "        processed_rows.append(processed_row)\n",
        "\n",
        "    # Convert results back to a DataFrame\n",
        "    df_processed = pd.DataFrame(processed_rows)\n",
        "\n",
        "    # Save the output to a new CSV file or overwrite the original file\n",
        "    df_processed.to_csv('easyocr.csv', index=False)\n",
        "    return df_processed"
      ],
      "metadata": {
        "id": "MqAR2lqcGAx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def easy_ocr(dataset):\n",
        "  process_csv_range_easy(dataset, start_row=0, end_row=131189)"
      ],
      "metadata": {
        "id": "o5bnq7eOFUGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def doc_tr(dataset):\n",
        "  # Initializing OCR\n",
        "  model = ocr_predictor(det_arch='db_resnet50', reco_arch='crnn_vgg16_bn', pretrained=True)\n",
        "  final_extract = []\n",
        "\n",
        "  # Process only the first 5 rows for testing\n",
        "  for i in tqdm(range(87458, 100000)):\n",
        "      img_url = dataset['image_link'][i]\n",
        "      response = requests.get(img_url)\n",
        "      if response.status_code == 200:\n",
        "          img = Image.open(BytesIO(response.content))\n",
        "          img.save(\"/kaggle/working/img1.jpg\")\n",
        "\n",
        "          try:\n",
        "              doc = DocumentFile.from_images(\"/kaggle/working/img1.jpg\")  # Wrap the image in a list\n",
        "              result = model(doc)\n",
        "              if result != [None]:\n",
        "                  l = []\n",
        "                  for page in result.pages:\n",
        "                      for block in page.blocks:\n",
        "                          for line in block.lines:\n",
        "                              for word in line.words:\n",
        "                                  l.append([word.value, word.confidence])\n",
        "              else:\n",
        "                  l = []\n",
        "\n",
        "          except SystemExit:\n",
        "              # Catch SystemExit exception and continue (prevents stopping the loop)\n",
        "              print(f\"Skipping image at index {i} due to SystemExit error.\")\n",
        "              l = []\n",
        "\n",
        "          except Exception as e:\n",
        "              # Catch any other exceptions and log the error without stopping\n",
        "              print(f\"Error processing image at index {i}: {str(e)}\")\n",
        "              print(traceback.format_exc())  # Optionally log the full traceback\n",
        "              l = []\n",
        "      else:\n",
        "          print(f\"Failed to fetch image: {img_url}\")\n",
        "          l = []\n",
        "\n",
        "      final_extract.append(l)\n",
        "\n",
        "  df = pd.DataFrame(list(zip(dataset['index'], dataset['image_link'][87458:], dataset['group_id'][87458:], dataset['entity_name'][87458:], dataset['entity_value'][87458:], final_extract)), columns = ['index', 'image_link', 'group_id', 'entity_name', 'entity_value', 'extracted_text'])\n",
        "  df.to_csv('doctr.csv', index=False)"
      ],
      "metadata": {
        "id": "dM5YXo_8G_xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('dataset/test.csv')\n",
        "dataset"
      ],
      "metadata": {
        "id": "TMGD39DkELxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paddle_ocr(dataset)\n",
        "easy_ocr(dataset)\n",
        "doc_tr(dataset)"
      ],
      "metadata": {
        "id": "Z7qUDwtfFSCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def object_detection_util(dataset):\n",
        "  entities_of_interest = ['height', 'width', 'depth']  # List of entities to filter\n",
        "  df = pd.read_csv(dataset)\n",
        "\n",
        "  # Filter rows where the 'entity_name' column matches any of the specified entities\n",
        "  filtered_df = df[df['entity_name'].isin(entities_of_interest)]\n",
        "\n",
        "  # Save the result to a new CSV file\n",
        "  output_file_path = 'height_width_depth_rows.csv'  # Replace with the desired output file path\n",
        "  filtered_df.to_csv(output_file_path, index=False)"
      ],
      "metadata": {
        "id": "dXf5UjZbKHhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image_from_url(image_url, model_path):\n",
        "    try:\n",
        "        # Fetch the image from the URL\n",
        "        response = requests.get(image_url)\n",
        "        img_array = np.asarray(bytearray(response.content), dtype=np.uint8)\n",
        "        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
        "\n",
        "        # Load YOLO model for this particular entity\n",
        "        model = YOLO(model_path)\n",
        "\n",
        "        # Perform inference with YOLO\n",
        "        results = model(img)\n",
        "\n",
        "        # Get bounding boxes in xyxy format\n",
        "        predictions = results[0].boxes  # Bounding box object\n",
        "\n",
        "        # Check if there are any bounding boxes\n",
        "        if len(predictions) > 0:\n",
        "            # Iterate through bounding boxes and apply OCR\n",
        "            ocr_results = []\n",
        "            for box in predictions:\n",
        "                # Get bounding box coordinates\n",
        "                x_min, y_min, x_max, y_max = map(int, box.xyxy[0])\n",
        "\n",
        "                # Crop the image using bounding box\n",
        "                cropped_img = img[y_min:y_max, x_min:x_max]\n",
        "\n",
        "                # Perform OCR on the cropped image\n",
        "                ocr_result = reader.readtext(cropped_img)\n",
        "\n",
        "                # Collect OCR text\n",
        "                text_output = ' '.join([text[1] for text in ocr_result])\n",
        "                ocr_results.append(text_output)\n",
        "\n",
        "            # Return concatenated OCR results\n",
        "            return ' '.join(ocr_results) if ocr_results else ''\n",
        "        else:\n",
        "            return ''  # No bounding boxes found\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_url}: {e}\")\n",
        "        return ''  # Return empty string on error\n"
      ],
      "metadata": {
        "id": "zQ3VXJjcKUOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def object_detection(dataset):\n",
        "  object_detection_util(dataset)\n",
        "  reader = easyocr.Reader(['en'])\n",
        "\n",
        "  # Load CSV file (replace 'image_link' with your actual image column name)\n",
        "  csv_file = 'height_width_depth_rows.csv'\n",
        "  df = pd.read_csv(csv_file)\n",
        "\n",
        "  # Add a new column for OCR results\n",
        "  df['ocr_output'] = ''\n",
        "\n",
        "  # Dictionary to hold YOLO models\n",
        "  model_paths = {\n",
        "      'height': 'height-depthweights/best_h.pt',\n",
        "      'width': 'height-depthweights/best.pt',\n",
        "      'depth': 'height-depthweights/best_d.pt'\n",
        "  }\n",
        "\n",
        "  for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    image_url = row['image_link']  # Replace 'image_link' with your actual column name\n",
        "    entity_name = row['entity_name']  # Replace 'entity_name' with your actual column name\n",
        "\n",
        "    # Get the appropriate model path based on the entity name\n",
        "    model_path = model_paths.get(entity_name, None)\n",
        "\n",
        "    if model_path:\n",
        "        ocr_text = process_image_from_url(image_url, model_path)\n",
        "        df.at[index, 'ocr_output'] = ocr_text\n",
        "    else:\n",
        "        print(f\"Model for entity '{entity_name}' not found. Skipping.\")\n",
        "        df.at[index, 'ocr_output'] = ''  # Optionally set empty string if model is not found\n",
        "\n",
        "  # Save the updated dataframe back to a new CSV\n",
        "  output_csv_file = 'detection.csv'\n",
        "  df.to_csv(output_csv_file, index=False)\n",
        "\n",
        "  print(\"OCR processing complete. Results saved to:\", output_csv_file)\n"
      ],
      "metadata": {
        "id": "A3scQ7goIK7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_cpm(dataset):\n",
        "  model = AutoModel.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5-int4', trust_remote_code=True)\n",
        "  tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5-int4', trust_remote_code=True)\n",
        "  model.eval()\n",
        "\n",
        "  i=0\n",
        "  image_name=\"test_image1.jpg\"\n",
        "  with open(dataset, 'r', newline='') as f, open(\"llm_output\", mode='w', newline='') as of:\n",
        "      reader = csv.reader(f)\n",
        "      writer = csv.writer(of)\n",
        "      next(reader)\n",
        "      for r in reader:\n",
        "          print(r[3])\n",
        "          res=\"NA\"\n",
        "          if r[3]!=\"height\" and r[3]!=\"width\" and r[3]!=\"depth\":\n",
        "              r.append(res)\n",
        "              writer.writerow(r)\n",
        "              continue\n",
        "          print(r[1])\n",
        "          url=r[1]\n",
        "          response = requests.get(url)\n",
        "          if response.status_code == 200:\n",
        "              # Create a file name from the URL\n",
        "              #image_name = os.path.join(save_dir, url.split('/')[-1])\n",
        "              # Save the image\n",
        "              with open(image_name, 'wb') as imf:\n",
        "                  imf.write(response.content)\n",
        "              print(f\"Image saved: {image_name}\")\n",
        "          else:\n",
        "              print(f\"Failed to fetch image: {url}\")\n",
        "\n",
        "          img= Image.open(image_name)\n",
        "          question = f'What is the {r[3]} as visible in the image ? Return along with proper units as visible in image. Return output in this format- {r[3]}=value units'\n",
        "          msgs = [{'role': 'user', 'content': question}]\n",
        "\n",
        "          try:\n",
        "              res = model.chat(\n",
        "                image= img,\n",
        "                msgs=msgs,\n",
        "                tokenizer=tokenizer,\n",
        "                sampling=False, # if sampling=False, beam_search will be used by default\n",
        "                temperature=0.3,\n",
        "                # system_prompt=system_prompt, # pass system_prompt if needed\n",
        "                num_beams = 4\n",
        "              )\n",
        "          except:\n",
        "              continue\n",
        "          i+=1\n",
        "          print(res)\n",
        "          r.append(res)\n",
        "          writer.writerow(r)\n",
        "          print(r)\n",
        "          print(f\"====================================================== {i} ==================================================\")\n",
        "  #         if i==10:\n",
        "  #             break"
      ],
      "metadata": {
        "id": "wU9fsDf5K8dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mini_cpm(dataset)"
      ],
      "metadata": {
        "id": "IAZRv4IeN8bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_ocr_output(test_csv, object_detect_csv, output_csv):\n",
        "    # Load both CSV files\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "    object_detect_df = pd.read_csv(object_detect_csv)\n",
        "\n",
        "    # Merge both dataframes on the 'index' column, using a left join to keep all rows from test_df\n",
        "    merged_df = pd.merge(test_df, object_detect_df[['index', 'ocr_output']], on='index', how='left')\n",
        "\n",
        "    # Save the updated test.csv\n",
        "    merged_df.to_csv(output_csv, index=False)\n",
        "\n",
        "# Example usage\n",
        "test_csv = 'test.csv'\n",
        "object_detect_csv = 'detection.csv'\n",
        "output_csv = 'updated_test.csv'\n",
        "\n",
        "merge_ocr_output(test_csv, object_detect_csv, output_csv)\n",
        "\n",
        "print(f\"Updated test.csv with ocr_output saved as {output_csv}\")"
      ],
      "metadata": {
        "id": "14QXA-AxQ1n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_ocr_output(updated_test_csv, minicpm_combined_csv, output_csv):\n",
        "    # Load both CSV files\n",
        "    updated_test_df = pd.read_csv(updated_test_csv)\n",
        "    minicpm_combined_df = pd.read_csv(minicpm_combined_csv)\n",
        "\n",
        "    # Merge both dataframes on 'index' column\n",
        "    merged_df = pd.merge(updated_test_df, minicpm_combined_df[['index', 'model response']], on='index', how='left')\n",
        "\n",
        "    # Replace 'ocr_output' with 'model response' wherever 'model response' is not NA\n",
        "    merged_df['ocr_output'] = merged_df.apply(\n",
        "        lambda row: row['model response'] if pd.notna(row['model response']) else row['ocr_output'], axis=1\n",
        "    )\n",
        "\n",
        "    # Drop the 'model response' column after updating\n",
        "    merged_df = merged_df.drop(columns=['model response'])\n",
        "\n",
        "    # Save the updated dataframe to a new CSV file\n",
        "    merged_df.to_csv(output_csv, index=False)\n",
        "\n",
        "# Example usage\n",
        "updated_test_csv = 'updated_test.csv'\n",
        "minicpm_combined_csv = 'llm_output.csv'\n",
        "output_csv = 'dimensions.csv'\n",
        "\n",
        "update_ocr_output(updated_test_csv, minicpm_combined_csv, output_csv)\n",
        "\n",
        "print(f\"Updated test.csv with model response saved as {output_csv}\")"
      ],
      "metadata": {
        "id": "9jEX81pyQ3vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_unit_map = {\n",
        "    'width': {'centimetre', 'cm', 'centimetres', 'foot', 'ft', 'feet', \"'\", 'inch', 'in', 'inches', '\"', 'metre', 'm', 'metres', 'millimetre', 'mm', 'millimetres', 'yard', 'yd', 'yards'},\n",
        "    'depth': {'centimetre', 'cm', 'centimetres', 'foot', 'ft', 'feet', \"'\", 'inch', 'in', 'inches', '\"', 'metre', 'm', 'metres', 'millimetre', 'mm', 'millimetres', 'yard', 'yd', 'yards'},\n",
        "    'height': {'centimetre', 'cm', 'centimetres', 'foot', 'ft', 'feet', \"'\", 'inch', 'in', 'inches', '\"', 'metre', 'm', 'metres', 'millimetre', 'mm', 'millimetres', 'yard', 'yd', 'yards'},\n",
        "    'item_weight': {'gram', 'g', 'grams', 'gs'\n",
        "        'kilogram', 'kg', 'kilograms', 'kgs',\n",
        "        'microgram', 'mcg', 'micrograms', 'mcgs',\n",
        "        'milligram', 'mg', 'milligrams', 'mgs',\n",
        "        'ounce', 'oz', 'ounces',\n",
        "        'pound', 'lb', 'pounds', 'lbs'},\n",
        "        # 'ton', 't', 'tons'},\n",
        "    'maximum_weight_recommendation': {'gram', 'g', 'grams', 'gs'\n",
        "        'kilogram', 'kg', 'kilograms', 'kgs',\n",
        "        'microgram', 'mcg', 'micrograms', 'mcgs',\n",
        "        'milligram', 'mg', 'milligrams', 'mgs',\n",
        "        'ounce', 'oz', 'ounces',\n",
        "        'pound', 'lb', 'pounds', 'lbs'},\n",
        "        # 'ton', 't', 'tons'},\n",
        "    'voltage': {'kilovolt', 'kv', 'kilovolts', 'millivolt', 'millivolts', 'mv', 'volt', 'volts', 'v'},\n",
        "    'wattage': {'kilowatt', 'kw', 'kilowatts', 'watt', 'w', 'watts'},\n",
        "    'item_volume': {'centilitre', 'cl', 'centilitres',\n",
        "        'cubic foot', 'cu ft', 'cubic feet', 'cuft', 'cu. ft.', 'cu.ft.',\n",
        "        'cubic inch', 'cu in', 'cubic inches', 'cuin', 'cu. in.', 'cu.in.',\n",
        "        'cup', 'cups',\n",
        "        'decilitre', 'dl', 'decilitres',\n",
        "        'fluid ounce', 'fl oz', 'fluid ounces', 'floz', 'fl. oz.', 'fl.oz.',\n",
        "        'ounce', 'oz', 'ounces',\n",
        "        'gallon', 'gal', 'gallons',\n",
        "        'imperial gallon', 'imp gal', 'imperial gallons', 'impgal', 'imp. gal.', 'imp.gal.',\n",
        "        'litre', 'l', 'litres',\n",
        "        'microlitre', 'mcl', 'microlitres',\n",
        "        'millilitre', 'ml', 'millilitres',\n",
        "        'pint', 'pt', 'pints',\n",
        "        'quart', 'qt', 'quarts'}\n",
        "}\n",
        "\n",
        "allowed_units = {unit for entity in entity_unit_map for unit in entity_unit_map[entity]}"
      ],
      "metadata": {
        "id": "gY-FO31ZRX60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "easy = pd.read_csv('easyocr.csv')\n",
        "paddle = pd.read_csv('paddleocr.csv')\n",
        "doctr = pd.read_excel('doctr.csv')\n",
        "replacements = pd.read_csv('dimensions.csv')\n",
        "\n",
        "i = 15\n",
        "row1e = easy['easy OCR'][i]\n",
        "row1p = paddle['paddle OCR'][i]\n",
        "row1d = doctr['doct OCR'][i]\n",
        "row1r = replacements['ocr_output'][i]\n",
        "\n",
        "# txt = ''\n",
        "\n",
        "if row1r != 'nan' and not (isinstance(row1r, float) and math.isnan(row1r)):\n",
        "  print('hehe')\n",
        "  txt = row1r\n",
        "else:\n",
        "  print('hi')\n",
        "  # Extract text and confidence scores using regular expressions\n",
        "  if isinstance(row1e, str):\n",
        "    matches = re.findall(r\"([^\\(\\)]+)\\s*\\((\\d+\\.\\d+)\\)\", row1e)\n",
        "    # Convert matches to a list of tuples\n",
        "    row1e = [(text.strip(), float(confidence)) for text, confidence in matches]\n",
        "  else:\n",
        "    row1e = []  # Handle cases where row1e is None or not a string\n",
        "  # Convert matches to a list of tuples\n",
        "  row1e = [(text.strip(), float(confidence)) for text, confidence in matches]\n",
        "  row1p = ast.literal_eval(row1p)\n",
        "  row1d = ast.literal_eval(row1d)\n",
        "\n",
        "\n",
        "  Firstrow1p = [item[0].lower() for item in row1p]\n",
        "  Firstrow1e = [item[0].lower() for item in row1e]\n",
        "  Firstrow1d = [item[0].lower() for item in row1d]\n",
        "  Firstrow1p.extend(Firstrow1e)\n",
        "  Firstrow1p.extend(Firstrow1d)\n",
        "  txt = set(Firstrow1p)\n",
        "  txt = ' '.join(txt)\n",
        "\n",
        "\n",
        "\n",
        "final_extract = []\n",
        "entity_name = easy['entity_name'][i]\n",
        "\n",
        "if entity_name != 'item_volume':\n",
        "    # Case like 1400mg, 16.5\", 5'\n",
        "    extracts = re.findall(r'\\d+\\.?\\d*[\"\\'a-zA-Z]+', txt)  # Added ' and \" symbols\n",
        "    for extract in extracts:\n",
        "        # Match where letters or ' or \" begin\n",
        "        match = re.search(r'[\"\\'a-zA-Z]+', extract)\n",
        "        if match and match.group(0) in entity_unit_map[entity_name]:\n",
        "            start_index = match.start()  # Index where letters or ' or \" begin\n",
        "            final_extract.append(extract)\n",
        "\n",
        "    # Case like 1400 mg, 16.5 \", 5 '\n",
        "    extracts = re.findall(r'\\d+\\.?\\d*\\s+[\"\\'a-zA-Z]+', txt)  # Added ' and \" symbols\n",
        "    for extract in extracts:\n",
        "        match = re.search(r'[\"\\'a-zA-Z]+', extract)\n",
        "        if match and match.group(0) in entity_unit_map[entity_name]:\n",
        "            start_index = match.start()  # Index where letters or ' or \" begin\n",
        "            final_extract.append(extract)\n",
        "else:\n",
        "\n",
        "  extracts = re.findall(r'\\d+\\.?\\d*[a-zA-Z]+', txt) #For cases like 14 cuft\n",
        "  for extract in extracts:\n",
        "    match = re.search(r'[a-zA-Z]+.*', extract) #Detects where letters begin\n",
        "    if match and match.group(0) in entity_unit_map[entity_name]:\n",
        "      final_extract.append(extract)\n",
        "\n",
        "  extracts = re.findall(r'\\d+\\.?\\d*[a-zA-Z]+\\s+[a-zA-Z]+', txt)\n",
        "  for extract in extracts:\n",
        "    match = re.search(r'[a-zA-Z]+.*', extract) #Detects where letters begin\n",
        "    if match and match.group(0) in entity_unit_map[entity_name]:\n",
        "      final_extract.append(extract)\n",
        "\n",
        "  extracts = re.findall(r'\\d+\\.?\\d*\\s+[a-zA-Z]+\\s+[a-zA-Z]+', txt)\n",
        "  for extract in extracts:\n",
        "    match = re.search(r'[a-zA-Z]+.*', extract) #Detects where letters begin\n",
        "    if match and match.group(0) in entity_unit_map[entity_name]:\n",
        "      final_extract.append(extract)\n",
        "\n",
        "  extracts = re.findall(r'\\d+\\.?\\d*\\s+[a-zA-Z]+', txt)\n",
        "  for extract in extracts:\n",
        "    match = re.search(r'[a-zA-Z]+.*', extract) #Detects where letters begin\n",
        "    if match and match.group(0) in entity_unit_map[entity_name]:\n",
        "      final_extract.append(extract)\n",
        "\n",
        "print('Final')\n",
        "print(list(set(final_extract)))\n",
        "\n",
        "# match = re.search(r'[a-zA-Z]+.*', final_extract[0]) #Detects where letters begin\n",
        "hehe = []\n",
        "for i in range(len(final_extract)):\n",
        "  match = re.search(r'[\"\\'a-zA-Z]+', final_extract[i])\n",
        "  num = final_extract[i][:match.start()]\n",
        "  final_unit = standardize_unit(match.group(0), easy['entity_name'][i])\n",
        "  final_op = num.strip() + ' ' + final_unit.strip()\n",
        "  hehe.append(final_op)"
      ],
      "metadata": {
        "id": "YHZz7jMpOEtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Define valid units and a dictionary of common OCR misreads\n",
        "valid_units = ['mg', 'ml', 'g', 'l', 'kg', 'oz', 'cm', 'mm']\n",
        "\n",
        "def correct_ocr_errors(misread_item):\n",
        "    # Separate the numeric part and the unit part using regex\n",
        "    import re\n",
        "\n",
        "    # Regex to match the number part (with possible decimal points) and the unit part\n",
        "    match = re.match(r'(\\d+\\.?\\d*)([a-zA-Z]+)', misread_item)\n",
        "\n",
        "    if not match:\n",
        "        return misread_item  # Return as is if it doesn't match the expected format\n",
        "\n",
        "    number_part = match.group(1)  # Extract the number part\n",
        "    unit_part = match.group(2)  # Extract the unit part\n",
        "\n",
        "    # Correct common OCR error: replace 'o' with '0' in the number part\n",
        "    corrected_number = unit_part.replace('o', '0')\n",
        "\n",
        "    # Use fuzzy matching to correct the unit part\n",
        "    best_match = None\n",
        "    best_ratio = 0\n",
        "\n",
        "    for unit in valid_units:\n",
        "        ratio = fuzz.ratio(unit_part.lower(), unit.lower())\n",
        "        if ratio > best_ratio:\n",
        "            best_ratio = ratio\n",
        "            best_match = unit\n",
        "\n",
        "    # If the best match is close enough (e.g., 70% similarity), correct the unit\n",
        "    if best_match and best_ratio > 70:\n",
        "        return corrected_number + best_match\n",
        "    else:\n",
        "        return corrected_number + unit_part  # If no good match, return as is\n",
        "\n",
        "# Example usage\n",
        "examples = ['140omg', '22oml', '500mg', '1L', '15oz', '14.5g']\n",
        "\n",
        "for item in examples:\n",
        "    corrected = correct_ocr_errors(item)\n",
        "    print(f\"Original: {item}, Corrected: {corrected}\")\n"
      ],
      "metadata": {
        "id": "VmVsof6WSHMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_ocr_errors(item, entity_name):\n",
        "  match = re.search(r'[a-zA-Z]+.*', item) #Detects where letters begin\n",
        "  if match.group(0) in entity_unit_map[entity_name]:\n",
        "    return item\n",
        "  else:\n",
        "    word_index = item.find(match.group(0)) #To find index of where letters begin\n",
        "    print('Word Index', word_index)\n",
        "\n",
        "    for unit in entity_unit_map[entity_name]: #To check for every unit in the dictionary\n",
        "      unit_index = (match.group(0)).find(unit)\n",
        "      if unit_index!=-1:\n",
        "        print('Unit', unit)\n",
        "        print('Unit Index', unit_index)\n",
        "        subtext = match.group(0)[:unit_index]\n",
        "        print(subtext)"
      ],
      "metadata": {
        "id": "X5RrOAroSKJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_unit(extracted_unit, entity_name):\n",
        "  if entity_name == 'item_weight' or entity_name == 'maximum_weight_recommendation':\n",
        "    if extracted_unit in ['gram', 'g', 'grams', 'gs']:\n",
        "      return 'gram'\n",
        "    elif extracted_unit in ['kilogram', 'kg', 'kilograms', 'kgs']:\n",
        "      return 'kilogram'\n",
        "    elif extracted_unit in ['microgram', 'mcg', 'micrograms', 'mcgs']:\n",
        "      return 'microgram'\n",
        "    elif extracted_unit in ['milligram', 'mg', 'milligrams', 'mgs']:\n",
        "      return 'milligram'\n",
        "    elif extracted_unit in ['ounce', 'oz', 'ounces']:\n",
        "      return 'ounce'\n",
        "    elif extracted_unit in ['pound', 'lb', 'pounds', 'lbs']:\n",
        "      return 'pound'\n",
        "    elif extracted_unit in ['ton', 't', 'tons']:\n",
        "      return 'ton'\n",
        "\n",
        "  elif entity_name == 'width' or entity_name == 'depth' or entity_name == 'height':\n",
        "    if extracted_unit in ['centimetre', 'cm', 'centimetres']:\n",
        "      return 'centimetre'\n",
        "    elif extracted_unit in ['foot', 'ft', 'feet', \"'\"]:\n",
        "      return 'foot'\n",
        "    elif extracted_unit in ['inch', 'in', 'inches', '\"']:\n",
        "      return 'inch'\n",
        "    elif extracted_unit in ['metre', 'm', 'metres']:\n",
        "      return 'metre'\n",
        "    elif extracted_unit in ['millimetre', 'mm', 'millimetres']:\n",
        "      return 'millimetre'\n",
        "    elif extracted_unit in ['yard', 'yd', 'yards']:\n",
        "      return 'yard'\n",
        "\n",
        "  elif entity_name == 'voltage':\n",
        "    if extracted_unit in ['kilovolt', 'kv', 'kilovolts']:\n",
        "      return 'kilovolt'\n",
        "    elif extracted_unit in ['millivolt', 'millivolts', 'mv']:\n",
        "      return 'millivolt'\n",
        "    elif extracted_unit in ['volt', 'volts', 'v']:\n",
        "      return 'volt'\n",
        "\n",
        "  elif entity_name == 'wattage':\n",
        "    if extracted_unit in ['kilowatt', 'kw', 'kilowatts']:\n",
        "      return 'kilowatt'\n",
        "    elif extracted_unit in ['watt', 'w', 'watts']:\n",
        "      return 'watt'\n",
        "\n",
        "  elif entity_name == 'item_volume':\n",
        "    if extracted_unit in ['centilitre', 'cl', 'centilitres']:\n",
        "      return 'centilitre'\n",
        "    elif extracted_unit in ['cubic foot', 'cu ft', 'cubic feet', 'cuft', 'cu. ft.', 'cu.ft.']:\n",
        "      return 'cubic foot'\n",
        "    elif extracted_unit in ['cubic inch', 'cu in', 'cubic inches', 'cuin', 'cu. in.', 'cu.in.']:\n",
        "      return 'cubic inch'\n",
        "    elif extracted_unit in ['cup', 'cups']:\n",
        "      return 'cup'\n",
        "    elif extracted_unit in ['decilitre', 'dl', 'decilitres']:\n",
        "      return 'decilitre'\n",
        "    elif extracted_unit in ['fluid ounce', 'fl oz', 'fluid ounces', 'floz', 'fl. oz.', 'fl.oz.', 'ounce', 'oz', 'ounces']:\n",
        "      return 'fluid ounce'\n",
        "    elif extracted_unit in ['gallon', 'gal', 'gallons']:\n",
        "      return 'gallon'\n",
        "    elif extracted_unit in ['imperial gallon', 'imp gal', 'imperial gallons', 'impgal', 'imp. gal.', 'imp.gal.']:\n",
        "      return 'imperial gallon'\n",
        "    elif extracted_unit in ['litre', 'l', 'litres']:\n",
        "      return 'litre'\n",
        "    elif extracted_unit in ['microlitre', 'mcl', 'microlitres']:\n",
        "      return 'microlitre'\n",
        "    elif extracted_unit in ['millilitre', 'ml', 'millilitres']:\n",
        "      return 'millilitre'\n",
        "    elif extracted_unit in ['pint', 'pt', 'pints']:\n",
        "      return 'pint'\n",
        "    elif extracted_unit in ['quart', 'qt', 'quarts']:\n",
        "      return 'quart'"
      ],
      "metadata": {
        "id": "y0LkiO8TSP8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_convertor(value):\n",
        "  num, unit = float(value.split()[0]), value.split()[1].lower()\n",
        "  conversion_factors = {\n",
        "    'gram': 1,\n",
        "    'kilogram': 1000,\n",
        "    'microgram': 1e-6,\n",
        "    'milligram': 1e-3,\n",
        "    'ounce': 28.3495,\n",
        "    'pound': 453.592,\n",
        "    'ton': 1e6,\n",
        "}\n",
        "  if unit in conversion_factors:\n",
        "      conversion_factor = conversion_factors[unit]\n",
        "      return num * conversion_factor\n",
        "\n",
        "def length_convertor(value):\n",
        "  num, unit = float(value.split()[0]), value.split()[1].lower()\n",
        "  conversion_factors = {\n",
        "    'centimetre': 10,\n",
        "    'foot': 304.8,\n",
        "    'inch': 25.4,\n",
        "    'metre': 1000,\n",
        "    'millimetre': 1,\n",
        "    'yard': 914.4,\n",
        "}\n",
        "  if unit in conversion_factors:\n",
        "    conversion_factor = conversion_factors[unit]\n",
        "    return num * conversion_factor\n",
        "\n",
        "def voltage_convertor(value):\n",
        "  num, unit = float(value.split()[0]), value.split()[1].lower()\n",
        "\n",
        "  if unit == 'volt':\n",
        "    return num * 1000\n",
        "  elif unit == 'millivolt':\n",
        "    return num\n",
        "  elif unit == 'kilovolt':\n",
        "    return num * 1000000\n",
        "\n",
        "def wattage_convertor(value):\n",
        "  num, unit = float(value.split()[0]), value.split()[1].lower()\n",
        "  if unit == 'watt':\n",
        "    return num\n",
        "  elif unit == 'kilowatt':\n",
        "    return num * 1000\n",
        "\n",
        "def volume_convertor(value):\n",
        "  parts = value.split()\n",
        "  num = float(parts[0])\n",
        "  unit = ' '.join(parts[1:]).lower()\n",
        "\n",
        "  conversion_factors = {\n",
        "    'centilitre': 10,\n",
        "    'cubic foot': 28316.8466,\n",
        "    'cubic inch': 16.3871,\n",
        "    'cup': 236.588,\n",
        "    'decilitre': 100,\n",
        "    'fluid ounce': 29.5735,\n",
        "    'ounce': 29.5735,\n",
        "    'gallon': 3785.41,\n",
        "    'imperial gallon': 4546.09,\n",
        "    'litre': 1000,\n",
        "    'microlitre': 0.001,\n",
        "    'millilitre': 1,\n",
        "    'pint': 473.176,\n",
        "    'quart': 946.353,\n",
        "}\n",
        "  if unit in conversion_factors:\n",
        "      conversion_factor = conversion_factors[unit]\n",
        "      return num * conversion_factor\n",
        "\n",
        "def optimum_op(extract_list, entity_name):\n",
        "  if entity_name == 'item_weight' or entity_name == 'maximum_weight_recommendation':\n",
        "    converted_values = [weight_convertor(value) for value in extract_list]\n",
        "    max_index = np.argmax(converted_values)\n",
        "    return extract_list[max_index]\n",
        "\n",
        "  elif entity_name == 'depth' or entity_name == 'height':\n",
        "    converted_values = [length_convertor(value) for value in extract_list]\n",
        "    max_index = np.argmax(converted_values)\n",
        "    return extract_list[max_index]\n",
        "\n",
        "  elif entity_name == 'width':\n",
        "    converted_values = [length_convertor(value) for value in extract_list]\n",
        "    min_index = np.argmin(converted_values)\n",
        "    return extract_list[min_index]\n",
        "\n",
        "  elif entity_name == 'voltage':\n",
        "    converted_values = [voltage_convertor(value) for value in extract_list]\n",
        "    max_index = np.argmax(converted_values)\n",
        "    return extract_list[max_index]\n",
        "\n",
        "  elif entity_name == 'wattage':\n",
        "    converted_values = [wattage_convertor(value) for value in extract_list]\n",
        "    max_index = np.argmax(converted_values)\n",
        "    return extract_list[max_index]\n",
        "\n",
        "  else:\n",
        "    converted_values = [volume_convertor(value) for value in extract_list]\n",
        "    max_index = np.argmax(converted_values)\n",
        "    return extract_list[max_index]"
      ],
      "metadata": {
        "id": "3n8dgPFCSW4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regex_op(txt, entity_name):\n",
        "  final_extract = []\n",
        "\n",
        "  if entity_name != 'item_volume':\n",
        "    # Case like 1400mg, 16.5\", 5'\n",
        "    extracts = re.findall(r'\\d+\\.?\\d*[\"\\'a-zA-Z]+', txt)  # Added ' and \" symbols\n",
        "    for extract in extracts:\n",
        "        # Match where letters or ' or \" begin\n",
        "        match = re.search(r'[\"\\'a-zA-Z]+', extract)\n",
        "        if match and match.group(0) in entity_unit_map[entity_name]:\n",
        "            start_index = match.start()  # Index where letters or ' or \" begin\n",
        "            final_extract.append(extract)\n",
        "\n",
        "    # Case like 1400 mg, 16.5 \", 5 '\n",
        "    extracts = re.findall(r'\\d+\\.?\\d*\\s+[\"\\'a-zA-Z]+', txt)  # Added ' and \" symbols\n",
        "    for extract in extracts:\n",
        "        match = re.search(r'[\"\\'a-zA-Z]+', extract)\n",
        "        if match and match.group(0) in entity_unit_map[entity_name]:\n",
        "            start_index = match.start()  # Index where letters or ' or \" begin\n",
        "            final_extract.append(extract)\n",
        "  else:\n",
        "\n",
        "    extracts = re.findall(r'\\d+\\.?\\d*[a-zA-Z]+', txt) #For cases like 14 cuft\n",
        "    for extract in extracts:\n",
        "      match = re.search(r'[a-zA-Z]+.*', extract) #Detects where letters begin\n",
        "      if match and match.group(0) in entity_unit_map[entity_name]:\n",
        "        final_extract.append(extract)\n",
        "\n",
        "    extracts = re.findall(r'\\d+\\.?\\d*[a-zA-Z]+\\s+[a-zA-Z]+', txt)\n",
        "    for extract in extracts:\n",
        "      match = re.search(r'[a-zA-Z]+.*', extract) #Detects where letters begin\n",
        "      if match and match.group(0) in entity_unit_map[entity_name]:\n",
        "        final_extract.append(extract)\n",
        "\n",
        "    extracts = re.findall(r'\\d+\\.?\\d*\\s+[a-zA-Z]+\\s+[a-zA-Z]+', txt)\n",
        "    for extract in extracts:\n",
        "      match = re.search(r'[a-zA-Z]+.*', extract) #Detects where letters begin\n",
        "      if match and match.group(0) in entity_unit_map[entity_name]:\n",
        "        final_extract.append(extract)\n",
        "\n",
        "    extracts = re.findall(r'\\d+\\.?\\d*\\s+[a-zA-Z]+', txt)\n",
        "    for extract in extracts:\n",
        "      match = re.search(r'[a-zA-Z]+.*', extract) #Detects where letters begin\n",
        "      if match and match.group(0) in entity_unit_map[entity_name]:\n",
        "        final_extract.append(extract)\n",
        "\n",
        "  return final_extract"
      ],
      "metadata": {
        "id": "jvqftv2jSYkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end_list = []\n",
        "for i in tqdm(range(len(easy))):\n",
        "  row1e = easy['easy OCR'][i]\n",
        "  row1p = paddle['paddle OCR'][i]\n",
        "  row1d = doctr['doct OCR'][i]\n",
        "  row1r = replacements['ocr_output'][i]\n",
        "\n",
        "  if row1r != 'nan' and not (isinstance(row1r, float) and math.isnan(row1r)):\n",
        "    txt = row1r\n",
        "  else:\n",
        "    # Extract text and confidence scores using regular expressions\n",
        "    if isinstance(row1e, str):\n",
        "      matches = re.findall(r\"([^\\(\\)]+)\\s*\\((\\d+\\.\\d+)\\)\", row1e)\n",
        "      # Convert matches to a list of tuples\n",
        "      row1e = [(text.strip(), float(confidence)) for text, confidence in matches]\n",
        "    else:\n",
        "      row1e = []  # Handle cases where row1e is None or not a string\n",
        "    # Convert matches to a list of tuples\n",
        "    try:\n",
        "            if pd.isna(row1p):  # If row1p is NaN or None\n",
        "                row1p = []  # Assign an empty list as a default\n",
        "            else:\n",
        "                row1p = ast.literal_eval(row1p)\n",
        "\n",
        "            if pd.isna(row1d):  # If row1d is NaN or None\n",
        "                row1d = []  # Assign an empty list as a default\n",
        "            else:\n",
        "                row1d = ast.literal_eval(row1d)\n",
        "    except (ValueError, SyntaxError) as e:\n",
        "        print(f\"Error parsing row1p or row1d on index {i}: {e}\")\n",
        "        row1p = []\n",
        "        row1d = []\n",
        "\n",
        "\n",
        "    Firstrow1p = [item[0].lower() for item in row1p]\n",
        "    Firstrow1e = [item[0].lower() for item in row1e]\n",
        "    Firstrow1d = [item[0].lower() for item in row1d]\n",
        "    Firstrow1p.extend(Firstrow1e)\n",
        "    Firstrow1p.extend(Firstrow1d)\n",
        "    txt = set(Firstrow1p)\n",
        "    txt = ' '.join(txt)\n",
        "\n",
        "\n",
        "  l = regex_op(txt, doctr['entity_name'][i])\n",
        "  final_l = []\n",
        "  for element in l:\n",
        "    match = re.search(r'[\"\\'a-zA-Z]+', element)\n",
        "    num = element[:match.start()]\n",
        "    unit = element[match.start():]\n",
        "    final_unit = standardize_unit(unit.strip(), doctr['entity_name'][i])\n",
        "    final_op = num.strip() + ' ' + final_unit.strip()\n",
        "    final_l.append(final_op)\n",
        "\n",
        "  if len(final_l) == 1:\n",
        "    end_list.append(final_l[0])\n",
        "  elif len(final_l) == 0:\n",
        "    end_list.append('')\n",
        "  else:\n",
        "    final_l = optimum_op(final_l, doctr['entity_name'][i])\n",
        "    end_list.append(final_l)"
      ],
      "metadata": {
        "id": "zBapnzdEW00m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `easy[:43730]` is a DataFrame or Series, and `end_list` is a list of predictions\n",
        "df = pd.DataFrame({\n",
        "    'index': easy['index'][:len(end_list)],  # If you want an index\n",
        "    'prediction': end_list  # Assuming end_list is the list of predictions\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('prediction.csv', index=False)"
      ],
      "metadata": {
        "id": "wL_Tiu9cW1lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.DataFrame({\n",
        "    'index': easy['index'][:len(end_list)],  # If you want an index\n",
        "    'image_link': easy['image_link'][:len(end_list)],\n",
        "    'group_id': easy['group_id'][:len(end_list)],\n",
        "    'entity_name': easy['entity_name'][:len(end_list)],\n",
        "    'prediction': end_list  # Assuming end_list is the list of predictions\n",
        "})\n",
        "df1.to_csv('Long_predictions.csv', index = False)"
      ],
      "metadata": {
        "id": "1lP8cygUW4ur"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}